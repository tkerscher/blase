{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blazar Synchrotron Tool (BlaST)\n",
    "\n",
    "BlaST is an ensemble of neural networks to estimate the synchrotron with prediction interval based of an blazar given its spectral energy distribution as output by the VOUBlazar tool. The goal of this notebook is to train the networks. This notebook wont feature any hyperparameter tuning since that already happened as part of my bachelor thesis.\n",
    "\n",
    "As a special quirk the tool uses double ensemble: Bagging is applied to create different subset to train. For each subset an ensemble is trained to improve the prediction as stated by Blundell et. al. 2017.\n",
    "\n",
    "In the end the bagging allows to reapply the model on the out of bag data to hopefully increase the quality of the dataset. The final tool is expected to be applied on unseen data and thus uses all available ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by parsing the data set. It is simply a zip containing seds as produced by the VOUBlazar tool and looks like the following:\n",
    "\n",
    "```\n",
    "   1  matched source  227.16230  -49.88400  99\n",
    " Frequency     nufnu     nufnu unc.  nufnu unc. start time   end time   Catalog     Reference\n",
    "    Hz       erg/cm2/s     upper       lower        MJD         MJD   \n",
    "---------------------------------------------------------------------------------------------------------------------------\n",
    " 2.418E+17   2.185E-13   3.139E-13   1.230E-13  55000.0000  55000.0000  RASS        Boller et al. 2016, A&A, 103, 1                                                                                                                                                                         \n",
    " 2.418E+17   5.085E-13   6.281E-13   3.889E-13  58150.0000  58150.1016  OUSXB       Giommi et al. 2019, Accepted for publication in A&A  \n",
    "```\n",
    "\n",
    "Thus, we have 4 lines of header until the actual data begins. After that we are only interested in the first 4 entries per row.\n",
    "The error is given for each direction separately, but would like a symmetric one better. We use the mean squarred error instead.\n",
    "\n",
    "Next, we need the target value, the nupeak. It's stored in the filename (as it was determined by hand). Consider an example file:\n",
    "```\n",
    "SED_11.78_227.1623_-49.8840_PMNJ1508-49_1.551\n",
    "```\n",
    "The peak is stored in the first float.\n",
    "\n",
    "Next, we have to sanitize the data. We encoutered following problems:\n",
    " - Zero Frequency\n",
    " - negative or zero flux\n",
    " - flux outside upper/lower bound\n",
    "\n",
    "Finally the data is binned. This is necessary since the neural network is a simple fcn and thus expects a constant sized input. There are gaps to leave out biased data. some bins are only present for specific target values. The neural network would only look if the bin is present and thus fail to generalize making them useless for unseen data. The actual bin edges were determined beforehand such that they are approximately equally densly filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import basename, splitext\n",
    "from zipfile import ZipFile\n",
    "\n",
    "data = []\n",
    "label = []\n",
    "pos = []\n",
    "seds = ZipFile('SEDs.zip')\n",
    "bin_edges = np.loadtxt('blase/bins.txt')\n",
    "n_bins = bin_edges.shape[0]\n",
    "\n",
    "#data loading\n",
    "def getpeak(filename):\n",
    "    filename = basename(filename)\n",
    "    return float(filename.split(\"_\")[1])\n",
    "def getpos(filename):\n",
    "    name = splitext(basename(filename))[0]\n",
    "    parts = name.split('_')\n",
    "    return float(parts[2]) ,float(parts[3])\n",
    "def sanitize(_data):\n",
    "    _data = np.delete(_data, _data[:,0] <= 0, axis=0)\n",
    "    _data = np.delete(_data, _data[:,1] <= 0, axis=0)\n",
    "    return _data\n",
    "def bin_data(_data):\n",
    "    result = []\n",
    "    for sed in _data:\n",
    "        line = []\n",
    "        for bin in bin_edges:\n",
    "            inside = (sed[:,0] >= bin[0]) & (sed[:,0] <= bin[1])\n",
    "            flux = sed[inside][:,1]\n",
    "            line.append(np.mean(flux) if len(flux) > 0 else 0.0)\n",
    "        result.append(line)\n",
    "    return np.array(result)\n",
    "def loadfile(file):\n",
    "    _data = []\n",
    "    for line in file.readlines()[4:]:\n",
    "        entries = line.split()\n",
    "        x = float(entries[0])\n",
    "        y = float(entries[1])\n",
    "        up = float(entries[2])\n",
    "        lo = float(entries[3])\n",
    "        #sanity check errors\n",
    "        if (up < y or y < lo) and up != 0.0 and lo != 0.0:\n",
    "            continue #Skip this entry\n",
    "        _data.append([x, y])\n",
    "    _data = np.array(_data)\n",
    "    _data = sanitize(_data)\n",
    "    #we want log10\n",
    "    _data = np.log10(_data)\n",
    "    assert(np.isfinite(_data).all())\n",
    "    return _data\n",
    "\n",
    "for filename in seds.namelist():\n",
    "    #Check if filename is a folder\n",
    "    if filename[-1] == '/':\n",
    "        continue\n",
    "    label.append(getpeak(filename))\n",
    "    pos.append(getpos(filename))\n",
    "    with seds.open(filename) as file:\n",
    "        data.append(loadfile(file))\n",
    "data = bin_data(data)\n",
    "label = np.array(label)\n",
    "pos = np.array(pos)\n",
    "\n",
    "print(f\"{len(data)} data entries loaded\")\n",
    "print(f\"{len(label)} labels loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create the bagging. There about 3,800 samples in the data set, thus each bag gets about 760 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to remember which sed is in which bag, thus we wont use standard methods but pick the bag ourselfes\n",
    "bag_idx = np.random.randint(5, size=len(label))\n",
    "bagged_data = [data[bag_idx == i] for i in range(5)]\n",
    "bagged_label = [label[bag_idx == i] for i in range(5)]\n",
    "for i in range(5):\n",
    "    print(f'Size of bag {i}: {len(bagged_label[i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save position with bag index. We need it later for autimatic checking wether a sed was used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('blast/bag_index.npy', np.hstack((pos, bag_idx.reshape(-1,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem of the data set is that not all synchrotron peaks and bins are equally represented. We solve this problem with data augmentation and thus evem create a bigger train set. The augmentation works by oversampling especially the seds with an uncommen peak while undersampling, i.e. deleting bins that are overrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_edges = np.loadtxt('label_edges.txt')\n",
    "_n_labels = len(label_edges) - 1\n",
    "target_bin_size = 90 #~10k training set size\n",
    "max_deletions = 15\n",
    "power_adjust = 4.0 #further boosts deletion probability of dense bins\n",
    "\n",
    "def augment(data, label):\n",
    "    #See which bins are actually filled\n",
    "    filled = np.where(data < 0.0)\n",
    "    hist, *_ = np.histogram2d(label[filled[0]], filled[1], bins=(label_edges, n_bins))\n",
    "\n",
    "    #calculate probality of bin to be deleted\n",
    "    p = (hist.T**power_adjust/np.sum(hist**power_adjust, axis=1)).T\n",
    "    a = np.array(list(range(n_bins)))\n",
    "    binned_label = np.digitize(label, label_edges, right=True) - 1#since bin zero gets 1\n",
    "\n",
    "    target_n = target_bin_size * _n_labels\n",
    "    result_data = np.zeros((target_n, n_bins))\n",
    "    result_label = np.zeros((target_n,))\n",
    "    #copy originals at back of output\n",
    "    result_data[-len(label):,:] = data\n",
    "    result_label[-len(label):] = label\n",
    "\n",
    "    for i in range(_n_labels): #binned labels\n",
    "        inside = np.where(binned_label == i)[0]\n",
    "        n = target_bin_size - len(inside) #nr to be copied\n",
    "        assert(n > 0)\n",
    "\n",
    "        #copy originals\n",
    "        _ii = target_bin_size*i\n",
    "        result_data[_ii:_ii+len(inside),:] = data[inside]\n",
    "        result_label[_ii:_ii+len(inside)] = label[inside]\n",
    "\n",
    "        #make copies\n",
    "        copy_sources = np.random.choice(inside, n) #which to copy (mutiple times)\n",
    "        _ii += len(inside)\n",
    "        result_data[_ii:_ii+n,:] = data[copy_sources]\n",
    "        result_label[_ii:_ii+n] = label[copy_sources]\n",
    "\n",
    "        #which bins to delete\n",
    "        for ii in range(_ii, _ii+n):\n",
    "            _del = np.random.choice(a, (max_deletions,), True, p[i])\n",
    "            result_data[ii, _del] = 0.0\n",
    "\n",
    "    #remove all copies with less than 5 bins populated\n",
    "    fainted = np.where((result_data != 0.0).sum(axis=1) < 5)\n",
    "    result_data = np.delete(result_data, fainted, axis=0)\n",
    "    result_label = np.delete(result_label, fainted, axis=0)\n",
    "    \n",
    "    return result_data, result_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = [augment(*b) for b in zip(bagged_data, bagged_label)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to make the data available for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "print(torch.__version__)\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance, we're gonna standardize the input per bin and the label. We use the whole augmented data set for that.\n",
    "\n",
    "Because of the standardization 0.0 becomes a valid flux and is thus ambigious for empty bins. Wether a bin is empty is therefore append after bins as a mask were zero denotes an empty and one a filled bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = np.concatenate([augmented[i][0] for i in range(5)])\n",
    "_label = np.concatenate([augmented[i][1] for i in range(5)])\n",
    "print(_data.shape)\n",
    "\n",
    "bin_mean = np.mean(_data, axis=0, where=(_data != 0.0))\n",
    "bin_scale = np.std(_data, axis=0, where=(_data != 0.0))\n",
    "\n",
    "label_mean = np.mean(_label, axis=0)\n",
    "label_scale = np.std(_label, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('blase/scaling.npz',\n",
    "    bin_mean=bin_mean,\n",
    "    bin_scale=bin_scale,\n",
    "    label_mean=label_mean,\n",
    "    label_scale=label_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, bag):\n",
    "        super().__init__()\n",
    "\n",
    "        _data = np.concatenate([augmented[i][0] for i in range(5) if i != bag])\n",
    "        _mask = (_data != 0.0).astype(float)\n",
    "        _label = np.concatenate([augmented[i][1] for i in range(5) if i != bag])\n",
    "\n",
    "        train_data, val_data, train_mask, val_mask, train_label, val_label = \\\n",
    "            train_test_split(_data, _mask, _label, test_size=1500)\n",
    "\n",
    "        self.train_data = (train_data - bin_mean) / bin_scale * train_mask\n",
    "        self.val_data = (val_data - bin_mean) / bin_scale * val_mask\n",
    "\n",
    "        self.train_mask = train_mask\n",
    "        self.val_mask = val_mask\n",
    "\n",
    "        self.train_label = (train_label - label_mean) / label_scale\n",
    "        self.val_label = (val_label - label_mean) / label_scale\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.trainSet = TensorDataset(\n",
    "            torch.tensor(np.concatenate((self.train_data, self.train_mask), axis=1), dtype=torch.float),\n",
    "            torch.tensor(self.train_label, dtype=torch.float))\n",
    "        self.valSet = TensorDataset(\n",
    "            torch.tensor(np.concatenate((self.val_data, self.val_mask), axis=1), dtype=float),\n",
    "            torch.tensor(self.val_label, dtype=float))\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainSet, batch_size=64, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valSet, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the model. Once again, the architecture has already been tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr = 0.0018397297637578883\n",
    "        self.weight_decay = 1.634587061861498e-05\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(52, 152),\n",
    "            nn.BatchNorm1d(152),\n",
    "            nn.Dropout(0.11623816061109485),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(152, 80),\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.Dropout(0.14953177977171542),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, 72),\n",
    "            nn.BatchNorm1d(72),\n",
    "            nn.Dropout(0.024569432237666035),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(72, 48),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.Dropout(0.03208157605345701),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.model(X.float())\n",
    "        #the second output is the variance\n",
    "        #use softplus to force the variance in [0,inf]\n",
    "        mean, var = torch.unbind(out, dim=1)#first axis is batch\n",
    "        var = F.softplus(var) #enforce > 0\n",
    "        return mean, var \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=10)\n",
    "        return {\n",
    "            'optimizer' : optim,\n",
    "            'scheduler' : scheduler\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y = y.squeeze()\n",
    "        mean, var = self(X)\n",
    "        #-2ln[p(y,x)]\n",
    "        losses = torch.log(var) + (y - mean)**2/var\n",
    "        loss = torch.mean(torch.unsqueeze(losses, 0)) #to keep losses comparable regardless of batch size\n",
    "        self.log('loss', loss)\n",
    "        return loss\n",
    "  \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y = y.squeeze()\n",
    "        mean, var = self(X)\n",
    "        losses = torch.log(var) + (y - mean)**2/var\n",
    "        #for whatever reason, mean needs an extra dimension...\n",
    "        loss = torch.mean(torch.unsqueeze(losses, 0))\n",
    "        self.log('val_loss', loss)\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        self.log('avg_loss', avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model can be trained. We'll train 5 models for each bag. This way we will have later an ensemble of 20 models for each out of bag estimation and even 25 for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size = 5\n",
    "\n",
    "for bag in range(5):\n",
    "    dataset = DataModule(bag)\n",
    "\n",
    "    for i in range(ensemble_size):\n",
    "        model = Model()\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=200,\n",
    "            logger = pl.loggers.TensorBoardLogger('logs/', name=f'bag{bag}', version=i),\n",
    "            progress_bar_refresh_rate=0,#disable progress bar\n",
    "            callbacks=[ModelCheckpoint(\n",
    "                dirpath='models/',\n",
    "                filename=f'{bag}.{i}_{{epoch}}_{{val_loss:.6f}}',\n",
    "                save_top_k=3,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                every_n_val_epochs=1)])\n",
    "        trainer.fit(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we saved checkpoints, we now have to extract the models weights to reduce disk footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import abspath, join\n",
    "\n",
    "models = []\n",
    "dic = {}\n",
    "\n",
    "for f in listdir('models/'):\n",
    "    m = Model.load_from_checkpoint(abspath(join('models/', f)))\n",
    "    models.append(m)\n",
    "    dic[f'{f[0:3]}'] = m.state_dict() #bag.id\n",
    "torch.save(dic, 'blase/models.pth') #save all models in one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is done, let's evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load blase\n",
    "from blast import Estimator\n",
    "\n",
    "estimator = Estimator()\n",
    "\n",
    "truth = label\n",
    "estimate = np.zeros_like(label)\n",
    "error = np.zeros_like(label)\n",
    "#estimate out of bag\n",
    "for i in range(5):\n",
    "    bag_mask = bag_idx == i\n",
    "    _estimate, _err = estimator(data[bag_mask], i)\n",
    "    estimate[bag_mask] = _estimate\n",
    "    error[bag_mask] = _err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate some metric\n",
    "print(f'Median Absolute Error: {np.median(np.abs(truth-estimate)):.3f}')\n",
    "print(f'         25% quantile: {np.quantile(np.abs(truth-estimate),0.25):.3f}')\n",
    "print(f'         75% quantile: {np.quantile(np.abs(truth-estimate),0.75):.3f}')\n",
    "print('')\n",
    "print(f'Median PI Width: {np.median(2*error):.3f}')\n",
    "print(f'   25% quantile: {np.quantile(2*error, 0.25):.3f}')\n",
    "print(f'   75% quantile: {np.quantile(2*error, 0.75):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "sns.set_theme()\n",
    "hist_cmap = sns.color_palette('PuBu', as_cmap=True)\n",
    "\n",
    "p = (np.min(label), np.max(label))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "_, _label_edges, *_ = plt.hist2d(truth, estimate, bins=30, cmap=hist_cmap)\n",
    "_bins = (_label_edges[:-1] + _label_edges[1:]) / 2\n",
    "_median, *_ = binned_statistic(truth, estimate, statistic='median', bins=_label_edges)\n",
    "_lower, *_ = binned_statistic(truth, estimate, bins=_label_edges, statistic=lambda x: np.percentile(x, 10))\n",
    "_upper, *_ = binned_statistic(truth, estimate, bins=_label_edges, statistic=lambda x: np.percentile(x, 90))\n",
    "plt.plot(p, p, color='gray', linewidth=1.35)\n",
    "plt.plot(_bins, _median, color='black', linewidth=1.2)\n",
    "plt.plot(_bins, _lower, color='black', linestyle='dashed', linewidth=1)\n",
    "plt.plot(_bins, _upper, color='black', linestyle='dashed', linewidth=1)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n",
    "\n",
    "sort = error.argsort()\n",
    "_estimate = estimate[sort]\n",
    "_error = error[sort]\n",
    "_truth = truth[sort] - _estimate\n",
    "N = len(truth)\n",
    "_below = (_truth < -_error).sum() / N * 100\n",
    "_above = (_truth > _error).sum() / N * 100\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.fill_between(np.arange(N), _error, -_error)\n",
    "plt.scatter(np.arange(N), _truth, s=2.5)\n",
    "plt.text(0,3,f'{_above:.2f}%')\n",
    "plt.text(0,-3,f'{_below:.2f}%')\n",
    "plt.ylabel('Prediction interval with ground truth (centered)')\n",
    "plt.xlabel('ordered samples')\n",
    "plt.ylim(-4,4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import splev, splrep\n",
    "\n",
    "sample_points = np.arange(_min, _max, 0.25)\n",
    "x = np.linspace(_min, _max, 200)\n",
    "linewidth = 1.5\n",
    "\n",
    "def moving_apply(data, f, width=0.5):\n",
    "    result = []\n",
    "    for s in sample_points:\n",
    "        mask = (truth >= s - width) & (truth <= s + width)\n",
    "        result.append(f(data[mask]))\n",
    "    return np.array(result)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "dense_ax = ax1.twinx()\n",
    "\n",
    "sns.kdeplot(truth, cut=0, ax=dense_ax, color='black', linestyle='dashed', linewidth=1)\n",
    "dense_ax.grid=False\n",
    "dense_ax.set(ylabel=None, yticks=[])\n",
    "\n",
    "mad = moving_apply(np.abs(truth - estimate), np.median)\n",
    "mad = splev(x, splrep(sample_points, mad))\n",
    "\n",
    "ax1.plot(x, mad)\n",
    "ax1.set_ylabel('Median Absolute Eror')\n",
    "ax1.set_xlabel('Log Synchrotron Peak [Hz]')\n",
    "ax1.set_yticks(np.linspace(0.1,0.7, 6))\n",
    "ax1.tick_params(axis='y', labelcolor='C0')\n",
    "ax1.set_axisbelow(True)\n",
    "\n",
    "pi = moving_apply(error, np.median)\n",
    "pi = splev(x, splrep(sample_points, pi))\n",
    "\n",
    "ax2.plot(x, pi, color='C1')\n",
    "ax2.set_ylabel('Median PI Width')\n",
    "ax2.set_yticks(np.linspace(0.7,1.0, 6))\n",
    "ax2.tick_params(axis='y', labelcolor='C1')\n",
    "ax2.set_axisbelow(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates from the avaluation section can actually be used to refine the data set. Since it is known to have some outliners it should be possible to reduce them through the generalization of this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print estimates as csv\n",
    "with open('estimates.csv', 'w') as csv:\n",
    "    csv.write('Right Ascension,Declination,Bag,Catalogue Peak,Estimated Peak,Estimate Error (95%)\\n')\n",
    "    for i in range(len(label)):\n",
    "        csv.write(f'{pos[i,0]},{pos[i,1]},{bag_idx[i]},{label[i]},{estimate[i]:.2f},{error[i]:.2f}\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
